"""Functions for managing DNNs"""

# pylint: disable=invalid-name

from pathlib import Path
from typing import Callable
import pandas as pd


def best_model(path: Path, metric: str):
    """
    Parses a logs.csv file generated by shrinkbench to get path to the best model from training.

    :param path: path to the root directory generated by shrinkbench, has a file called logs.csv
    :param metric: how to compare models to find the best one, it is {train_, val_} +
        {loss, acc1, acc5}
    :return: a path to the file that stores the model with the best metric
    """

    if not isinstance(path, Path):
        path = Path(path)

    csv_path = path / "logs.csv"
    df = pd.read_csv(csv_path)
    df = df[df["epoch"] >= 0]
    best_epoch = None

    if "loss" in metric:
        best_epoch = int(df[df[metric] == df[metric].min()]["epoch"].iloc[0])
    if "acc" in metric:
        best_epoch = int(df[df[metric] == df[metric].max()]["epoch"].iloc[0])

    return path / "checkpoints" / f"checkpoint-{best_epoch}.pt"


def get_hyperparameters(
    model_type: str, num_workers: int = 4, debug: int = None
) -> (dict, dict):
    """
    Returns the training and pruning hyperparameters for a model

    :param model_type: architecture of the model.
    :param num_workers: number of worker threads to use to load a dataset.
    :param debug: if specified, it is the number of epochs for training and
        finetuning

    :return: a tuple of (kwargs for training, kwargs for testing)
    """

    # for now all models use resnet parameters
    if "resnet" in model_type or 1:
        model_type = "resnet"

    model_kwargs = {
        "resnet": {
            "train": {
                "optim": "SGD",
                "epochs": 300 if not debug else debug,
                "lr": 1e-1,
                "weight_decay": 1e-4,
            },
            "prune": {
                "optim": "SGD",
                "epochs": 40 if not debug else debug,
                "lr": 1e-3,
                "weight_decay": 1e-4,
            },
        }
    }

    # kwargs used in both training and fine-tuning
    common_kwargs = {
        "dl_kwargs": {
            "batch_size": 128,
            "pin_memory": False,
            "num_workers": num_workers,
        },
        "save_freq": 1000,
        "early_stop_method": None,
    }

    train_kwargs = {
        **common_kwargs,
        "train_kwargs": {**model_kwargs[model_type]["train"]},
        "lr_schedule": get_lr_schedule(
            model_kwargs[model_type]["train"]["lr"],
            model_kwargs[model_type]["train"]["epochs"],
        ),
    }

    # use a fixed lr for finetuning
    prune_kwargs = {
        **common_kwargs,
        "train_kwargs": {**model_kwargs[model_type]["prune"]},
        "lr_schedule": get_lr_schedule(
            model_kwargs[model_type]["prune"]["lr"],
            model_kwargs[model_type]["prune"]["epochs"],
            "fixed",
        ),
    }

    return train_kwargs, prune_kwargs


def get_lr_schedule(
    intial_rate: float, num_epochs: int, method: str = None
) -> Callable:
    """
    Factory function to generate a learning rate schedule.

    :param intial_rate: the initial learning rate.
    :param num_epochs: the number of epochs used for training.
    :param method: the lr method.

    :return: a function with signature (epochs: int) that returns the learning
        rate given an epoch.
    """

    def lr_schedule(epoch: int) -> float:
        """Given an epoch, get the learning rate."""
        if epoch < num_epochs * 0.5:
            return intial_rate
        if num_epochs * 0.75 > epoch >= num_epochs * 0.5:
            return intial_rate / 10
        return intial_rate / 100

    if method == "fixed":
        return lambda x: intial_rate
    return lr_schedule
