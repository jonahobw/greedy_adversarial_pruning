"""
Model management utilities.

Provides functions for hyperparameter selection, best checkpoint retrieval,
and model architecture handling.
"""

# pylint: disable=invalid-name

from pathlib import Path
from typing import Callable
import pandas as pd


def best_model(path: Path, metric: str = None):
    """
    Parses a logs.csv file generated by shrinkbench to get path to the best model from training.

    Args:
        path (Path or str): Path to the root directory generated by shrinkbench, has a file called logs.csv.
        metric (str, optional): How to compare models to find the best one. It is {train_, val_} +
            {loss, acc1, acc5}. If there is no metric provided, takes the model from the last epoch.
    Returns:
        Path: Path to the file that stores the model with the best metric.
    """
    if not isinstance(path, Path):
        path = Path(path)

    csv_path = path / "logs.csv"
    df = pd.read_csv(csv_path)
    df = df[df["epoch"] >= 0]
    best_epoch = None

    if metric is not None:
        if "loss" in metric:
            best_epoch = int(df[df[metric] == df[metric].min()]["epoch"].iloc[0])
        if "acc" in metric:
            best_epoch = int(df[df[metric] == df[metric].max()]["epoch"].iloc[0])
    else:
        best_epoch = int(df["epoch"].max())

    return path / "checkpoints" / f"checkpoint-{best_epoch}.pt"


def get_hyperparameters(
    model_type: str, num_workers: int = 4, debug: int = None
) -> (dict, dict):
    """
    Returns the training and pruning hyperparameters for a model.

    Args:
        model_type (str): Architecture of the model.
        num_workers (int): Number of worker threads to use to load a dataset.
        debug (int, optional): If specified, it is the number of epochs for training and finetuning.

    Returns:
        tuple: (kwargs for training, kwargs for pruning)
    """
    # For now all models use resnet parameters
    if "resnet" in model_type or 1:
        model_type = "resnet"

    model_kwargs = {
        "resnet": {
            "train": {
                "optim": "SGD",
                "epochs": 300 if not debug else debug,
                "lr": 1e-1,
                "weight_decay": 1e-4,
            },
            "prune": {
                "optim": "SGD",
                "epochs": 40 if not debug else debug,
                "lr": 1e-3,
                "weight_decay": 1e-4,
            },
        }
    }

    # kwargs used in both training and fine-tuning
    common_kwargs = {
        "dl_kwargs": {
            "batch_size": 128,
            "pin_memory": False,
            "num_workers": num_workers,
        },
        "save_freq": 1000,
        "early_stop_method": None,
    }

    train_kwargs = {
        **common_kwargs,
        "train_kwargs": {**model_kwargs[model_type]["train"]},
        "lr_schedule": get_lr_schedule(
            model_kwargs[model_type]["train"]["lr"],
            model_kwargs[model_type]["train"]["epochs"],
        ),
    }

    # Use a fixed lr for finetuning
    prune_kwargs = {
        **common_kwargs,
        "train_kwargs": {**model_kwargs[model_type]["prune"]},
        "lr_schedule": get_lr_schedule(
            model_kwargs[model_type]["prune"]["lr"],
            model_kwargs[model_type]["prune"]["epochs"],
            "fixed",
        ),
    }

    return train_kwargs, prune_kwargs


def get_lr_schedule(
    intial_rate: float, num_epochs: int, method: str = None
) -> Callable:
    """
    Factory function to generate a learning rate schedule.

    Args:
        intial_rate (float): The initial learning rate.
        num_epochs (int): The number of epochs used for training.
        method (str, optional): The lr method. If 'fixed', returns a constant schedule.

    Returns:
        Callable: A function with signature (epoch: int) that returns the multiplication factor
            for the initial learning rate given the epoch.
    """
    def lr_schedule(epoch: int) -> float:
        """
        Given an epoch, get the multiplication factor for the learning rate.

        Keeps initial lr until halfway done with training, when lr decreases by a factor of 10.
        Then lr becomes 1/100 of initial lr at 75% of the way done with training.
        """
        if epoch < num_epochs * 0.5:
            return 1
        if num_epochs * 0.75 > epoch >= num_epochs * 0.5:
            return 0.1
        return 0.01

    if method == "fixed":
        return lambda x: intial_rate
    return lr_schedule
